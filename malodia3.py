# -*- coding: utf-8 -*-
"""malodia3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gtnW8s57keFRGiAK0afMAFBLi_HJWO_q
"""

import os
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

!pip install PyDrive

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

download = drive.CreateFile({'id': '1S4i1UcZLhU0OD4Myt10NnFrX2cB0uRQk'})

download.GetContentFile('train_SOaYf6mv.zip')
!unzip train_SOaYf6mv.zip

from keras.models import Sequential
get_ipython().magic('matplotlib inline')
import matplotlib.pyplot as plt
import numpy as np
import keras
from keras.layers import Dense
import pandas as pd

from keras.applications.vgg16 import VGG16
from keras.preprocessing import image
from keras.applications.vgg16 import preprocess_input
import numpy as np
from keras.applications.vgg16 import decode_predictions
from keras.utils.np_utils import to_categorical

from sklearn.preprocessing import LabelEncoder
from keras.models import Sequential
from keras.optimizers import SGD
from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation

from sklearn.metrics import log_loss
from tqdm import tqdm

train = pd.read_csv('train.csv')

train_img=[]
for i in tqdm(range(train.shape[0])):
  temp_img = image.load_img('images/'+train['image_names'][i], target_size=(224,224), grayscale=False)
  #temp_img= image.load_img(train_path+ '/'+str(i), target_size=(28,28),grayscale = False)
  temp_img=image.img_to_array(temp_img)
  train_img.append(temp_img)

from keras.models import Model

def vgg16_model(img_rows, img_cols, channel=1, num_classes=None):
    

   model = VGG16(weights='imagenet', include_top=True)

   model.layers.pop()

   model.outputs = [model.layers[-1].output]

   model.layers[-1].outbound_nodes = []

   x=Dense(num_classes, activation='sigmoid')(model.output)

   model=Model(model.input,x)

#To set the first 8 layers to non-trainable (weights will not be updated)

   for layer in model.layers[:8]:
       layer.trainable = False

# Learning rate is changed to 0.001
       sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)
       model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])

       return model

train_y = train['emergency_or_not'].values
le = LabelEncoder()

train_y = le.fit_transform(train_y)

train_y=to_categorical(train_y)
train_y=np.array(train_y)

from sklearn.model_selection import train_test_split
X_train, X_valid, Y_train, Y_valid=train_test_split(train_img,train_y,test_size=0.2, random_state=42)

# Example to fine-tune on 3000 samples from Cifar10

img_rows, img_cols = 224, 224 # Resolution of inputs
channel = 3
num_classes = 2 
batch_size = 32
nb_epoch = 30

X_train = np.stack(X_train)
X_valid = np.stack(X_valid)
Y_train = np.stack(Y_train)
Y_valid = np.stack(Y_valid)

# Load our model
model = vgg16_model(img_rows, img_cols, channel, num_classes)

# Start Fine-tuning
model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Make predictions
predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)

score = log_loss(Y_valid, predictions_valid)
score

c = np.argmax(predictions_valid, axis=1)
c

#test_set
download = drive.CreateFile({'id': '12AWKqOC1zriCd1vBNEw5SY1bWTBmaiMT'})
download.GetContentFile('test_set.zip')
!unzip test_set.zip

test = pd.read_csv('test_set.csv')

test_image = []
for i in tqdm(range(test.shape[0])):
    img = image.load_img('images/'+test['image_names'][i], target_size=(224,224), grayscale=False)
    img = image.img_to_array(img)
    test_image.append(img)

test_image = np.stack(test_image)
test_image.shape

# Make predictions
predictions_valid2 = model.predict(test_image, batch_size=batch_size, verbose=1)

predictions_valid2

d = np.argmax(predictions_valid2, axis=1)
d

download = drive.CreateFile({'id': '14zMkps-8XpA3yRdrrdFqLnuxZiKMooHQ'})
download.GetContentFile('sample_submission_yxjOnvz.csv')

# creating submission file
sample = pd.read_csv('sample_submission_yxjOnvz.csv')
sample['emergency_or_not'] = d
sample.to_csv('Malodia5.csv', header=True, index=False)

